# 1. Importing Libraries 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import joblib
import warnings
warnings.filterwarnings("ignore")

#  2 . Load Dataset 
data = pd.read_csv("student_performance.csv")

# Quick look at the dataset
print("First 5 rows:")
print(data.head())
print("\nDataset Info:")
print(data.info())
print("\nSummary Statistics:")
print(data.describe())

# ---- STEP 3: Data Cleaning ----
# Check for missing values
print("\nMissing values per column:")
print(data.isnull().sum())
# 3. Data Cleaning
print("\nMissing values per column:")
print(data.isnull().sum())
# 4. EDA and Target Selection
# Correlation heatmap (numeric only)
plt.figure(figsize=(12,8))
sns.heatmap(
    data.select_dtypes(include=["number"]).corr(),
    annot=True, cmap="coolwarm"
)
plt.title("Correlation Heatmap (Numeric Features Only)")
plt.show()

# Target Selection
X = data[["Hours_Studied"]]   # Feature(s)
y = data["Exam_Score"]        # Target
# Scatterplot
plt.figure(figsize=(6,4))
sns.scatterplot(x="Hours_Studied", y="Exam_Score", data=data)
plt.title("Study Hours vs Exam Score")
plt.xlabel("Study Hours")
plt.ylabel("Exam Score")
plt.show()


# 5. Model Training
# Correlation bar plot with Exam Score
corr_with_target = (
    data.select_dtypes(include=["number"])
    .corr()["Exam_Score"]
    .sort_values(ascending=False)
)

plt.figure(figsize=(8,6))
sns.barplot(
    x=corr_with_target.values,
    y=corr_with_target.index,
    palette="viridis"
)
plt.title("Correlation of Features with Exam Score")
plt.xlabel("Correlation Coefficient")
plt.ylabel("Features")
plt.show()



# Single-feature regression
X_single = data[["Hours_Studied"]]
y = data["Exam_Score"]
# Multi-feature regression (bonus)
X_multi = data[["Hours_Studied", "Attendance", "Sleep_Hours", "Previous_Scores"]]


X_train, X_test, y_train, y_test = train_test_split(
    X_single, y, test_size=0.2, random_state=42
)

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

y_pred = lr_model.predict(X_test)
# 6. Model Evaluation
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\nðŸ“Š Linear Regression (Single Feature) Performance:")
print(f"MAE:  {mae:.2f}")
print(f"MSE:  {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"RÂ²:   {r2:.2f}")
plt.figure(figsize=(6,4))
plt.scatter(y_test, y_pred, alpha=0.7, color="blue")
plt.plot([y.min(), y.max()], [y.min(), y.max()], "r--")
plt.xlabel("Actual Scores")
plt.ylabel("Predicted Scores")
plt.title("Actual vs Predicted Exam Scores (Linear Regression)")
plt.show()
# 7. Polynomial Regression (BONUS)
poly = PolynomialFeatures(degree=2)
X_poly_train = poly.fit_transform(X_train)
X_poly_test = poly.transform(X_test)

poly_model = LinearRegression()
poly_model.fit(X_poly_train, y_train)

y_poly_pred = poly_model.predict(X_poly_test)

mae_poly = mean_absolute_error(y_test, y_poly_pred)
mse_poly = mean_squared_error(y_test, y_poly_pred)
rmse_poly = np.sqrt(mse_poly)
r2_poly = r2_score(y_test, y_poly_pred)

print("\nðŸ“Š Polynomial Regression (Single Feature) Performance:")
print(f"MAE:  {mae_poly:.2f}")
print(f"MSE:  {mse_poly:.2f}")
print(f"RMSE: {rmse_poly:.2f}")
print(f"RÂ²:   {r2_poly:.2f}")

plt.figure(figsize=(6,4))
plt.scatter(X_single, y, color="blue", alpha=0.5, label="Data")
X_range = np.linspace(X_single.min(), X_single.max(), 100).reshape(-1, 1)
plt.plot(X_range, lr_model.predict(X_range), color="red", label="Linear Regression")
plt.plot(X_range, poly_model.predict(poly.transform(X_range)), color="green", label="Polynomial Regression")
plt.xlabel("Hours Studied")
plt.ylabel("Exam Score")
plt.title("Linear vs Polynomial Regression")
plt.legend()
plt.show()
# 8. Multi-Feature Regression (BONUS)
X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(
    X_multi, y, test_size=0.2, random_state=42
)

multi_model = LinearRegression()
multi_model.fit(X_train_multi, y_train_multi)

y_multi_pred = multi_model.predict(X_test_multi)

mae_multi = mean_absolute_error(y_test_multi, y_multi_pred)
r2_multi = r2_score(y_test_multi, y_multi_pred)

print("\nðŸ“Š Linear Regression (Multi-Feature) Performance:")
print(f"MAE:  {mae_multi:.2f}")
print(f"RÂ²:   {r2_multi:.2f}")

joblib.dump(lr_model, "linear_regression_model.pkl")
joblib.dump(poly_model, "polynomial_regression_model.pkl")
joblib.dump(multi_model, "multi_feature_regression_model.pkl")
